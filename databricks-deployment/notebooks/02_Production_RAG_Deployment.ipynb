{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production RAG System - Model Deployment\n",
        "\n",
        "**Architecture:**\n",
        "- ðŸ” **BGE Model**: `databricks-bge-large-en` for high-quality embeddings and retrieval\n",
        "- ðŸ§  **Llama 3.3 70B**: `databricks-meta-llama-3-3-70b-instruct` for reasoning and response generation\n",
        "- ðŸš€ **MLflow 3.0**: For production deployment and serving\n",
        "- ðŸ“Š **Vector Search**: `jgmt` endpoint with `icc.jugement.main_text_summarized` index\n",
        "\n",
        "**Deployment Target:** Production serving endpoint for ICC judgment Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -U -qqqq mlflow>=3.1.1 langchain langgraph databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv databricks-feature-engineering==0.12.1\n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "import sys\n",
        "sys.path.append('/Workspace/Users/christophe629@gmail.com/icc_rag_backend/databricks-deployment/config')\n",
        "\n",
        "# Import unified configuration\n",
        "from databricks_config import *\n",
        "\n",
        "# Core imports\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass, asdict\n",
        "import datetime\n",
        "import logging\n",
        "\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "from mlflow.models.resources import (\n",
        "    DatabricksVectorSearchIndex,\n",
        "    DatabricksServingEndpoint\n",
        ")\n",
        "\n",
        "# Vector Search and LLM\n",
        "from databricks.vector_search.client import VectorSearchClient\n",
        "from langchain_community.chat_models import ChatDatabricks\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "print(\"âœ… Configuration and dependencies loaded\")\n",
        "print_config_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Production RAG System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RetrievalContext:\n",
        "    chunk_id: str\n",
        "    content: str\n",
        "    summary: str\n",
        "    section_type: str\n",
        "    page_range: str\n",
        "    similarity_score: float\n",
        "    relevance_score: float\n",
        "\n",
        "class ProductionRAGSystem:\n",
        "    \"\"\"Production RAG system with BGE retrieval and Llama conversation.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Initialize clients with configuration\n",
        "        self.vsc = VectorSearchClient()\n",
        "        self.llm = ChatDatabricks(\n",
        "            target_uri=\"databricks\",\n",
        "            endpoint=LLAMA_MODEL_ENDPOINT,\n",
        "            temperature=RAG_CONFIG[\"temperature\"],\n",
        "            max_tokens=RAG_CONFIG[\"max_tokens\"]\n",
        "        )\n",
        "        \n",
        "        # Use legal expansions from config\n",
        "        self.legal_expansions = LEGAL_EXPANSIONS\n",
        "        self.person_entities = KEY_ENTITIES[\"persons\"]\n",
        "        \n",
        "        # Conversation memory\n",
        "        self.conversations = {}\n",
        "    \n",
        "    def enhance_query(self, query: str) -> str:\n",
        "        \"\"\"Enhance query for better BGE retrieval.\"\"\"\n",
        "        enhanced = query.lower()\n",
        "        \n",
        "        # Add legal term expansions from config\n",
        "        for term, expansions in self.legal_expansions.items():\n",
        "            if term in enhanced:\n",
        "                enhanced += f\" {expansions[0]}\"\n",
        "        \n",
        "        # Add person context\n",
        "        for person in self.person_entities:\n",
        "            if person.lower() in enhanced:\n",
        "                enhanced += f\" defendant {person}\"\n",
        "        \n",
        "        return enhanced\n",
        "    \n",
        "    def retrieve_contexts(self, query: str, num_results: int = None) -> List[RetrievalContext]:\n",
        "        \"\"\"Retrieve contexts using BGE-powered vector search.\"\"\"\n",
        "        if num_results is None:\n",
        "            num_results = RAG_CONFIG[\"default_num_results\"]\n",
        "            \n",
        "        try:\n",
        "            # Enhance query for BGE\n",
        "            enhanced_query = self.enhance_query(query)\n",
        "            \n",
        "            # Perform vector search\n",
        "            results = self.vsc.get_index(VECTOR_SEARCH_ENDPOINT, VECTOR_SEARCH_INDEX).similarity_search(\n",
        "                query_text=enhanced_query,\n",
        "                columns=[\"chunk_id\", \"content\", \"section_type\", \"page_range\", \"summary\"],\n",
        "                num_results=num_results * 2  # Get extra for filtering\n",
        "            )\n",
        "            \n",
        "            docs = results.get('result', {}).get('data_array', [])\n",
        "            \n",
        "            # Convert to RetrievalContext objects\n",
        "            contexts = []\n",
        "            for doc in docs:\n",
        "                if len(doc) >= 6:\n",
        "                    section_weight = get_section_weight(doc[2])\n",
        "                    context = RetrievalContext(\n",
        "                        chunk_id=doc[0],\n",
        "                        content=doc[1],\n",
        "                        section_type=doc[2],\n",
        "                        page_range=doc[3],\n",
        "                        summary=doc[4],\n",
        "                        similarity_score=float(doc[5]),\n",
        "                        relevance_score=float(doc[5]) * section_weight\n",
        "                    )\n",
        "                    contexts.append(context)\n",
        "            \n",
        "            # Apply relevance boosting\n",
        "            contexts = self._boost_relevance(contexts, query)\n",
        "            \n",
        "            # Sort and return top results\n",
        "            contexts = sorted(contexts, key=lambda x: x.relevance_score, reverse=True)\n",
        "            return contexts[:num_results]\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Retrieval error: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def _boost_relevance(self, contexts: List[RetrievalContext], query: str) -> List[RetrievalContext]:\n",
        "        \"\"\"Apply relevance boosting based on data insights.\"\"\"\n",
        "        query_lower = query.lower()\n",
        "        \n",
        "        for context in contexts:\n",
        "            boost = 1.0\n",
        "            text = f\"{context.content} {context.summary}\".lower()\n",
        "            \n",
        "            # Person mention boost (key insight: 723 Yekatom mentions)\n",
        "            for person in self.person_entities:\n",
        "                if person.lower() in text:\n",
        "                    boost += 0.15\n",
        "            \n",
        "            # Legal concept boost\n",
        "            legal_terms = [\"war crime\", \"murder\", \"persecution\", \"command\"]\n",
        "            for term in legal_terms:\n",
        "                if term in text:\n",
        "                    boost += 0.10\n",
        "            \n",
        "            context.relevance_score *= boost\n",
        "        \n",
        "        return contexts\n",
        "    \n",
        "    def generate_response(self, query: str, contexts: List[RetrievalContext], \n",
        "                         conversation_id: str = None) -> str:\n",
        "        \"\"\"Generate response using Llama with legal expertise.\"\"\"\n",
        "        \n",
        "        # Get or create conversation memory\n",
        "        if conversation_id and RAG_CONFIG[\"enable_conversation_memory\"]:\n",
        "            if conversation_id not in self.conversations:\n",
        "                self.conversations[conversation_id] = ConversationBufferWindowMemory(\n",
        "                    k=RAG_CONFIG[\"conversation_memory_window\"], \n",
        "                    return_messages=True\n",
        "                )\n",
        "            memory = self.conversations[conversation_id]\n",
        "            history = memory.chat_memory.messages\n",
        "        else:\n",
        "            history = []\n",
        "        \n",
        "        # Format contexts for Llama\n",
        "        context_text = self._format_contexts(contexts)\n",
        "        \n",
        "        # Create legal system prompt\n",
        "        system_prompt = f\"\"\"You are an expert legal analyst specializing in International Criminal Court (ICC) proceedings. You are analyzing the judgment in \"{DOCUMENT_INFO['case_name']}\" ({DOCUMENT_INFO['case_number']}).\n",
        "\n",
        "Your expertise includes:\n",
        "- International criminal law (war crimes, crimes against humanity, genocide)\n",
        "- ICC procedures and legal standards\n",
        "- Central African Republic conflict analysis\n",
        "- Legal reasoning and evidence evaluation\n",
        "\n",
        "Guidelines:\n",
        "1. Base responses strictly on the provided judgment context\n",
        "2. Use proper legal terminology and cite specific sections\n",
        "3. Maintain judicial objectivity\n",
        "4. Reference page numbers and sections when available\n",
        "5. Clearly state when information is not available in the context\n",
        "\n",
        "Structure your response with clear legal reasoning and specific evidence.\"\"\"\n",
        "\n",
        "        # Create the prompt\n",
        "        chat_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"history\"),\n",
        "            (\"human\", \"\"\"\n",
        "Context from ICC Judgment:\n",
        "{context}\n",
        "\n",
        "Legal Question: {query}\n",
        "\n",
        "Please provide a comprehensive legal analysis based on the judgment context provided.\"\"\")\n",
        "        ])\n",
        "        \n",
        "        try:\n",
        "            # Format messages\n",
        "            messages = chat_template.format_messages(\n",
        "                context=context_text,\n",
        "                query=query,\n",
        "                history=history\n",
        "            )\n",
        "            \n",
        "            # Generate response\n",
        "            response = self.llm(messages)\n",
        "            \n",
        "            # Update memory if conversation_id provided\n",
        "            if conversation_id and RAG_CONFIG[\"enable_conversation_memory\"]:\n",
        "                memory.chat_memory.add_user_message(query)\n",
        "                memory.chat_memory.add_ai_message(response.content)\n",
        "            \n",
        "            return response.content\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "    \n",
        "    def _format_contexts(self, contexts: List[RetrievalContext]) -> str:\n",
        "        \"\"\"Format contexts for Llama processing.\"\"\"\n",
        "        if not contexts:\n",
        "            return \"No relevant context found.\"\n",
        "        \n",
        "        formatted = []\n",
        "        for i, ctx in enumerate(contexts, 1):\n",
        "            formatted.append(f\"\"\"\n",
        "**Source {i}** - {ctx.section_type} (Pages {ctx.page_range})\n",
        "Relevance: {ctx.relevance_score:.3f}\n",
        "\n",
        "Content: {ctx.content}\n",
        "\n",
        "Summary: {ctx.summary}\n",
        "---\"\"\")\n",
        "        \n",
        "        return \"\\n\".join(formatted)\n",
        "    \n",
        "    def process_query(self, query: str, num_results: int = None, conversation_id: str = None) -> Dict:\n",
        "        \"\"\"Process complete RAG query.\"\"\"\n",
        "        start_time = datetime.datetime.now()\n",
        "        \n",
        "        # Retrieve contexts\n",
        "        contexts = self.retrieve_contexts(query, num_results)\n",
        "        \n",
        "        # Generate response\n",
        "        response = self.generate_response(query, contexts, conversation_id)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        processing_time = (datetime.datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            \"response\": response,\n",
        "            \"conversation_id\": conversation_id,\n",
        "            \"num_contexts\": len(contexts),\n",
        "            \"processing_time_seconds\": processing_time,\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"chunk_id\": ctx.chunk_id,\n",
        "                    \"section\": ctx.section_type,\n",
        "                    \"pages\": ctx.page_range,\n",
        "                    \"relevance\": round(ctx.relevance_score, 3)\n",
        "                }\n",
        "                for ctx in contexts[:5]  # Top 5 sources\n",
        "            ] if RAG_CONFIG[\"include_sources\"] else []\n",
        "        }\n",
        "\n",
        "# Initialize the system\n",
        "rag_system = ProductionRAGSystem()\n",
        "print(\"âœ… Production RAG System initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLflow Model and Deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the RAG system locally\n",
        "def test_rag_system():\n",
        "    \"\"\"Test the RAG system with sample queries.\"\"\"\n",
        "    print(\"ðŸ§ª TESTING PRODUCTION RAG SYSTEM\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    test_queries = [\n",
        "        \"What war crimes was Alfred Yekatom found guilty of?\",\n",
        "        \"What evidence supported the persecution charges?\",\n",
        "        \"What sentence was imposed on Yekatom?\"\n",
        "    ]\n",
        "    \n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\n{i}. Query: {query}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        try:\n",
        "            result = rag_system.process_query(\n",
        "                query=query,\n",
        "                num_results=8,\n",
        "                conversation_id=f\"test_{i}\"\n",
        "            )\n",
        "            \n",
        "            print(f\"âœ… Response length: {len(result['response'])} chars\")\n",
        "            print(f\"ðŸ“Š Contexts used: {result['num_contexts']}\")\n",
        "            print(f\"â±ï¸  Processing time: {result['processing_time_seconds']:.2f}s\")\n",
        "            print(f\"ðŸ“ Preview: {result['response'][:150]}...\")\n",
        "            \n",
        "            if result['sources']:\n",
        "                print(f\"ðŸ” Top source: {result['sources'][0]['section']} (relevance: {result['sources'][0]['relevance']})\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "    \n",
        "    print(\"\\nâœ… RAG system testing complete\")\n",
        "\n",
        "# Run tests\n",
        "test_rag_system()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLflow Model Wrapper\n",
        "class ProductionRAGModel(mlflow.pyfunc.PythonModel):\n",
        "    \"\"\"MLflow 3.0 production model wrapper.\"\"\"\n",
        "    \n",
        "    def load_context(self, context):\n",
        "        \"\"\"Initialize the production RAG system.\"\"\"\n",
        "        self.rag_system = ProductionRAGSystem()\n",
        "    \n",
        "    def predict(self, context, model_input: pd.DataFrame) -> List[Dict]:\n",
        "        \"\"\"Handle predictions for serving endpoint.\"\"\"\n",
        "        try:\n",
        "            queries = model_input[\"query\"].tolist()\n",
        "            \n",
        "            # Extract optional parameters\n",
        "            num_results_list = model_input.get(\"num_results\", [RAG_CONFIG[\"default_num_results\"]] * len(queries)).tolist()\n",
        "            conversation_ids = model_input.get(\"conversation_id\", [None] * len(queries)).tolist()\n",
        "            \n",
        "            results = []\n",
        "            for query, num_results, conv_id in zip(queries, num_results_list, conversation_ids):\n",
        "                try:\n",
        "                    # Process query\n",
        "                    result = self.rag_system.process_query(\n",
        "                        query=query,\n",
        "                        num_results=num_results if pd.notna(num_results) else RAG_CONFIG[\"default_num_results\"],\n",
        "                        conversation_id=conv_id if pd.notna(conv_id) else None\n",
        "                    )\n",
        "                    results.append(result)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    # Handle individual query errors\n",
        "                    error_result = {\n",
        "                        \"response\": f\"Error processing query: {str(e)}\",\n",
        "                        \"conversation_id\": conv_id,\n",
        "                        \"num_contexts\": 0,\n",
        "                        \"processing_time_seconds\": 0,\n",
        "                        \"sources\": []\n",
        "                    }\n",
        "                    results.append(error_result)\n",
        "            \n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            return [{\"error\": f\"Model error: {str(e)}\"}] * len(model_input)\n",
        "\n",
        "# Register the production model\n",
        "with mlflow.start_run(run_name=\"ICC_RAG_Production_BGE_Llama\") as run:\n",
        "    \n",
        "    # Create model instance\n",
        "    production_model = ProductionRAGModel()\n",
        "    \n",
        "    # Input example for serving endpoint\n",
        "    input_example = pd.DataFrame({\n",
        "        \"query\": [\n",
        "            \"What role did Alfred Yekatom play in the Anti-Balaka forces?\",\n",
        "            \"What evidence was presented about persecution of Muslims?\"\n",
        "        ],\n",
        "        \"num_results\": [10, 12],\n",
        "        \"conversation_id\": [\"session_001\", \"session_001\"]\n",
        "    })\n",
        "    \n",
        "    # Expected output format\n",
        "    output_example = [\n",
        "        {\n",
        "            \"response\": \"Based on the ICC judgment...\",\n",
        "            \"conversation_id\": \"session_001\",\n",
        "            \"num_contexts\": 10,\n",
        "            \"processing_time_seconds\": 2.5,\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"chunk_id\": \"chunk_0001\",\n",
        "                    \"section\": \"EVIDENTIARY_CONSIDERATIONS\",\n",
        "                    \"pages\": \"150-151\",\n",
        "                    \"relevance\": 0.95\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Log the model\n",
        "    mlflow.pyfunc.log_model(\n",
        "        artifact_path=\"icc_rag_bge_llama_model\",\n",
        "        python_model=production_model,\n",
        "        input_example=input_example,\n",
        "        signature=infer_signature(input_example, output_example),\n",
        "        resources=[\n",
        "            DatabricksVectorSearchIndex(index_name=VECTOR_SEARCH_INDEX),\n",
        "            DatabricksServingEndpoint(endpoint_name=BGE_MODEL_ENDPOINT),\n",
        "            DatabricksServingEndpoint(endpoint_name=LLAMA_MODEL_ENDPOINT)\n",
        "        ],\n",
        "        pip_requirements=[\n",
        "            \"mlflow>=3.1.1\",\n",
        "            \"langchain\",\n",
        "            \"databricks-langchain\",\n",
        "            \"numpy\",\n",
        "            \"pandas\"\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Register model\n",
        "    model_uri = f\"runs:/{run.info.run_id}/icc_rag_bge_llama_model\"\n",
        "    registered_model = mlflow.register_model(\n",
        "        model_uri=model_uri,\n",
        "        name=get_databricks_path(\"rag_model\")\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… Model logged: {run.info.run_id}\")\n",
        "    print(f\"ðŸ“¦ Model registered: {registered_model.name} v{registered_model.version}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ PRODUCTION RAG DEPLOYMENT COMPLETE!\")\n",
        "print(f\"ðŸ“¦ Model: {get_databricks_path('rag_model')}\")\n",
        "print(\"ðŸš€ Ready for serving endpoint deployment!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
