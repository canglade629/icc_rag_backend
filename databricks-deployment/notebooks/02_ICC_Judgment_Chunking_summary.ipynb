{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Read the Parquet file into a DataFrame\n",
        "chunks_df = spark.read.parquet(\"/Volumes/icc/jugement/files/chunks_data.parquet\")\n",
        "\n",
        "# Step 2: Write the DataFrame as a managed Delta table\n",
        "chunks_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"icc.jugement.chunks\")\n",
        "\n",
        "# Step 3: Display the first few rows to confirm\n",
        "chunks_table = spark.table(\"chunks\")\n",
        "display(chunks_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"ALTER TABLE icc.jugement.chunks SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Read the table as a DataFrame\n",
        "chunks_df = spark.table(\"icc.jugement.chunks\")\n",
        "\n",
        "# Add the summary column using ai_summarize (limit to 200 tokens if needed)\n",
        "chunks_with_summary = chunks_df.withColumn(\n",
        "    \"summary\", expr(\"ai_summarize(content, 200)\")\n",
        ")\n",
        "\n",
        "# Overwrite the table with the new DataFrame\n",
        "chunks_with_summary.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"icc.jugement.chunks\")\n",
        "\n",
        "# Display a sample of the updated table\n",
        "display(spark.table(\"icc.jugement.chunks\").select(\"chunk_id\", \"content\", \"summary\").limit(10))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
