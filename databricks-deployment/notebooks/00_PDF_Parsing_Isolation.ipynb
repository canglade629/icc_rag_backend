{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Parsing Isolation - ICC Judgment Processing\n",
        "\n",
        "This notebook isolates the PDF parsing logic from the main RAG pipeline, providing a dedicated, reusable PDF processing system for ICC judgments.\n",
        "\n",
        "## üéØ Purpose\n",
        "- **Isolated PDF parsing** using PyMuPDF (winner from local testing)\n",
        "- **Modular design** for easy integration with other notebooks\n",
        "- **Production-ready** with comprehensive error handling\n",
        "- **Optimized for legal documents** with ICC-specific patterns\n",
        "\n",
        "## üìä Library Comparison Results\n",
        "Based on local testing with the actual ICC judgment:\n",
        "- **PyMuPDF**: 28.8x faster, 4,475,751 chars extracted\n",
        "- **pdfplumber**: 141.7s extraction time, 4,407,110 chars extracted\n",
        "- **Winner**: PyMuPDF for speed and text quality\n",
        "\n",
        "## üîÑ Workflow Integration\n",
        "This notebook is designed to be the first step in the pipeline:\n",
        "1. **Parse PDF** ‚Üí This notebook\n",
        "2. **Build Chunks** ‚Üí 03_ICC_Judgment_Chunking.ipynb\n",
        "3. **Summarize Chunks** ‚Üí 04_ICC_Judgment_Chunking_summary.ipynb\n",
        "4. **Optimize Vector Search** ‚Üí 01_Optimized_Vector_Search.ipynb\n",
        "5. **Build and Deploy RAG** ‚Üí 02_Production_RAG_Deployment.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install PyMuPDF pandas pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration and imports\n",
        "import sys\n",
        "sys.path.append('/Workspace/Users/christophe629@gmail.com/icc_rag_backend/databricks-deployment/config')\n",
        "\n",
        "# Import unified configuration\n",
        "from databricks_config import *\n",
        "\n",
        "# Core imports\n",
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple, Optional, Set\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import logging\n",
        "import builtins\n",
        "\n",
        "# Spark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "print(\"‚úÖ Configuration and imports loaded\")\n",
        "print_config_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Models for PDF Parsing\n",
        "\n",
        "Structured data models to maintain consistency and type safety across the parsing pipeline:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ParsedPage:\n",
        "    \"\"\"Represents a single parsed page from the PDF.\"\"\"\n",
        "    page_number: int\n",
        "    text: str\n",
        "    text_length: int\n",
        "    word_count: int\n",
        "    section_type: str\n",
        "    has_images: bool\n",
        "    image_count: int\n",
        "    extraction_quality: float  # 0.0-1.0\n",
        "    processing_time: float  # seconds\n",
        "\n",
        "@dataclass\n",
        "class LegalParagraph:\n",
        "    \"\"\"Represents a legal paragraph with numbering.\"\"\"\n",
        "    paragraph_id: str\n",
        "    paragraph_number: Optional[str]  # [123] format\n",
        "    content: str\n",
        "    page_number: int\n",
        "    section_type: str\n",
        "    word_count: int\n",
        "    char_count: int\n",
        "    is_numbered: bool\n",
        "    extraction_confidence: float\n",
        "\n",
        "@dataclass\n",
        "class DocumentMetadata:\n",
        "    \"\"\"Document-level metadata extracted from PDF.\"\"\"\n",
        "    case_name: str\n",
        "    case_number: str\n",
        "    chamber: str\n",
        "    date: str\n",
        "    total_pages: int\n",
        "    total_text_length: int\n",
        "    total_word_count: int\n",
        "    extraction_method: str\n",
        "    processing_time: float\n",
        "    quality_score: float\n",
        "\n",
        "@dataclass\n",
        "class ParsedDocument:\n",
        "    \"\"\"Complete parsed document structure.\"\"\"\n",
        "    metadata: DocumentMetadata\n",
        "    pages: List[ParsedPage]\n",
        "    paragraphs: List[LegalParagraph]\n",
        "    sections: Dict[str, List[int]]  # section_type -> page_numbers\n",
        "    extraction_errors: List[str]\n",
        "\n",
        "print(\"‚úÖ Data models defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ICC PDF Parser Class\n",
        "\n",
        "Production-ready PDF parser optimized for ICC judgments using PyMuPDF:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ICCPDFParser:\n",
        "    \"\"\"Production PDF parser optimized for ICC judgments using PyMuPDF.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict = None):\n",
        "        \"\"\"Initialize parser with configuration.\"\"\"\n",
        "        self.config = config or {}\n",
        "        \n",
        "        # ICC-specific patterns for section identification\n",
        "        self.section_patterns = [\n",
        "            ('OVERVIEW', r'I\\.\\s+OVERVIEW'),\n",
        "            ('FINDINGS_OF_FACT', r'II\\.\\s+FINDINGS\\s+OF\\s+FACT'),\n",
        "            ('EVIDENTIARY_CONSIDERATIONS', r'III\\.\\s+EVIDENTIARY\\s+CONSIDERATIONS'),\n",
        "            ('APPLICABLE_LAW', r'IV\\.\\s+APPLICABLE\\s+LAW'),\n",
        "            ('LEGAL_CHARACTERISATION', r'V\\.\\s+LEGAL\\s+CHARACTERISATION'),\n",
        "            ('SENTENCE', r'VI\\.\\s+SENTENCE'),\n",
        "            ('VERDICT', r'VII\\.\\s+VERDICT'),\n",
        "        ]\n",
        "        \n",
        "        # Legal paragraph patterns\n",
        "        self.paragraph_patterns = [\n",
        "            r'\\[(\\d+)\\]\\s*([^[]*?)(?=\\[|\\Z)',  # [123] format\n",
        "            r'^\\s*(\\d{1,3})\\.\\s+([^0-9].*?)(?=\\n\\s*\\d+\\.|\\Z)',  # 123. format\n",
        "        ]\n",
        "        \n",
        "        # Document metadata patterns\n",
        "        self.metadata_patterns = {\n",
        "            'case_name': r'Prosecutor\\s+v\\.\\s+([^,]+)',\n",
        "            'case_number': r'(ICC-\\d+/\\d+)',\n",
        "            'chamber': r'(Trial\\s+Chamber\\s+[IVX]+|Appeals\\s+Chamber)',\n",
        "            'date': r'(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n",
        "        }\n",
        "        \n",
        "        # Quality assessment patterns\n",
        "        self.quality_indicators = {\n",
        "            'legal_paragraphs': r'\\[\\d+\\]',\n",
        "            'case_citations': r'ICC-\\d+/\\d+',\n",
        "            'section_headers': r'^[IVX]+\\.\\s+[A-Z\\s]+$',\n",
        "            'legal_terms': r'(war\\s+crime|crimes\\s+against\\s+humanity|genocide|persecution)',\n",
        "        }\n",
        "    \n",
        "    def parse_pdf(self, pdf_path: str) -> ParsedDocument:\n",
        "        \"\"\"Parse PDF and return structured document.\"\"\"\n",
        "        print(f\"üîç Parsing PDF: {pdf_path}\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Open PDF with PyMuPDF\n",
        "            doc = fitz.open(pdf_path)\n",
        "            \n",
        "            # Extract document metadata\n",
        "            metadata = self._extract_document_metadata(doc, pdf_path)\n",
        "            \n",
        "            # Parse all pages\n",
        "            pages = []\n",
        "            paragraphs = []\n",
        "            sections = defaultdict(list)\n",
        "            errors = []\n",
        "            \n",
        "            for page_num in range(doc.page_count):\n",
        "                try:\n",
        "                    page_start = time.time()\n",
        "                    page = doc[page_num]\n",
        "                    \n",
        "                    # Extract page text\n",
        "                    text = page.get_text(\"text\")\n",
        "                    \n",
        "                    # Identify section type\n",
        "                    section_type = self._identify_section_type(text)\n",
        "                    sections[section_type].append(page_num + 1)\n",
        "                    \n",
        "                    # Extract images info\n",
        "                    images = page.get_images()\n",
        "                    has_images = len(images) > 0\n",
        "                    \n",
        "                    # Calculate extraction quality\n",
        "                    quality = self._calculate_page_quality(text)\n",
        "                    \n",
        "                    # Create page object\n",
        "                    parsed_page = ParsedPage(\n",
        "                        page_number=page_num + 1,\n",
        "                        text=text,\n",
        "                        text_length=len(text),\n",
        "                        word_count=len(text.split()),\n",
        "                        section_type=section_type,\n",
        "                        has_images=has_images,\n",
        "                        image_count=len(images),\n",
        "                        extraction_quality=quality,\n",
        "                        processing_time=time.time() - page_start\n",
        "                    )\n",
        "                    pages.append(parsed_page)\n",
        "                    \n",
        "                    # Extract legal paragraphs\n",
        "                    page_paragraphs = self._extract_legal_paragraphs(text, page_num + 1, section_type)\n",
        "                    paragraphs.extend(page_paragraphs)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Page {page_num + 1}: {str(e)}\"\n",
        "                    errors.append(error_msg)\n",
        "                    print(f\"‚ö†Ô∏è  {error_msg}\")\n",
        "            \n",
        "            # Update metadata with actual counts\n",
        "            metadata.total_pages = len(pages)\n",
        "            metadata.total_text_length = sum(p.text_length for p in pages)\n",
        "            metadata.total_word_count = sum(p.word_count for p in pages)\n",
        "            metadata.processing_time = time.time() - start_time\n",
        "            metadata.quality_score = self._calculate_document_quality(pages, paragraphs)\n",
        "            \n",
        "            # Create parsed document\n",
        "            parsed_doc = ParsedDocument(\n",
        "                metadata=metadata,\n",
        "                pages=pages,\n",
        "                paragraphs=paragraphs,\n",
        "                sections=dict(sections),\n",
        "                extraction_errors=errors\n",
        "            )\n",
        "            \n",
        "            doc.close()\n",
        "            \n",
        "            print(f\"‚úÖ PDF parsing complete: {len(pages)} pages, {len(paragraphs)} paragraphs\")\n",
        "            print(f\"‚è±Ô∏è  Processing time: {metadata.processing_time:.2f}s\")\n",
        "            print(f\"üéØ Quality score: {metadata.quality_score:.2f}/100\")\n",
        "            \n",
        "            return parsed_doc\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå PDF parsing failed: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def _extract_document_metadata(self, doc: fitz.Document, pdf_path: str) -> DocumentMetadata:\n",
        "        \"\"\"Extract document-level metadata.\"\"\"\n",
        "        # Try to extract from first few pages\n",
        "        metadata_text = \"\"\n",
        "        for page_num in range(builtins.min(3, doc.page_count)):\n",
        "            page = doc[page_num]\n",
        "            metadata_text += page.get_text(\"text\") + \"\\n\"\n",
        "        \n",
        "        # Extract using patterns\n",
        "        case_name = \"Unknown Case\"\n",
        "        case_number = \"Unknown\"\n",
        "        chamber = \"Unknown Chamber\"\n",
        "        date = \"Unknown Date\"\n",
        "        \n",
        "        for pattern_name, pattern in self.metadata_patterns.items():\n",
        "            match = re.search(pattern, metadata_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                if pattern_name == 'case_name':\n",
        "                    case_name = match.group(1).strip()\n",
        "                elif pattern_name == 'case_number':\n",
        "                    case_number = match.group(1).strip()\n",
        "                elif pattern_name == 'chamber':\n",
        "                    chamber = match.group(1).strip()\n",
        "                elif pattern_name == 'date':\n",
        "                    date = match.group(1).strip()\n",
        "        \n",
        "        return DocumentMetadata(\n",
        "            case_name=case_name,\n",
        "            case_number=case_number,\n",
        "            chamber=chamber,\n",
        "            date=date,\n",
        "            total_pages=0,  # Will be updated later\n",
        "            total_text_length=0,  # Will be updated later\n",
        "            total_word_count=0,  # Will be updated later\n",
        "            extraction_method=\"PyMuPDF\",\n",
        "            processing_time=0.0,  # Will be updated later\n",
        "            quality_score=0.0  # Will be updated later\n",
        "        )\n",
        "    \n",
        "    def _identify_section_type(self, text: str) -> str:\n",
        "        \"\"\"Identify the section type based on text content.\"\"\"\n",
        "        text_upper = text.upper()\n",
        "        \n",
        "        for section_name, pattern in self.section_patterns:\n",
        "            if re.search(pattern, text_upper):\n",
        "                return section_name\n",
        "        \n",
        "        # Check for common headers\n",
        "        if any(header in text_upper for header in ['TRIAL CHAMBER', 'APPEALS CHAMBER']):\n",
        "            return 'HEADER'\n",
        "        \n",
        "        return 'UNKNOWN'\n",
        "    \n",
        "    def _extract_legal_paragraphs(self, text: str, page_number: int, section_type: str) -> List[LegalParagraph]:\n",
        "        \"\"\"Extract legal paragraphs from page text.\"\"\"\n",
        "        paragraphs = []\n",
        "        \n",
        "        for pattern in self.paragraph_patterns:\n",
        "            for match in re.finditer(pattern, text, re.DOTALL):\n",
        "                para_num = match.group(1)\n",
        "                para_content = match.group(2).strip()\n",
        "                \n",
        "                if len(para_content) > 20:  # Skip very short paragraphs\n",
        "                    # Clean content\n",
        "                    clean_content = re.sub(r'\\s+', ' ', para_content).strip()\n",
        "                    \n",
        "                    # Calculate confidence\n",
        "                    confidence = self._calculate_paragraph_confidence(clean_content)\n",
        "                    \n",
        "                    paragraph = LegalParagraph(\n",
        "                        paragraph_id=f\"para_{section_type}_{page_number}_{para_num}\",\n",
        "                        paragraph_number=para_num,\n",
        "                        content=clean_content,\n",
        "                        page_number=page_number,\n",
        "                        section_type=section_type,\n",
        "                        word_count=len(clean_content.split()),\n",
        "                        char_count=len(clean_content),\n",
        "                        is_numbered=True,\n",
        "                        extraction_confidence=confidence\n",
        "                    )\n",
        "                    paragraphs.append(paragraph)\n",
        "        \n",
        "        return paragraphs\n",
        "    \n",
        "    def _calculate_page_quality(self, text: str) -> float:\n",
        "        \"\"\"Calculate extraction quality for a page.\"\"\"\n",
        "        score = 0.0\n",
        "        \n",
        "        # Check for legal document indicators\n",
        "        for indicator, pattern in self.quality_indicators.items():\n",
        "            matches = len(re.findall(pattern, text, re.IGNORECASE))\n",
        "            if matches > 0:\n",
        "                score += builtins.min(matches * 0.1, 0.3)  # Cap at 0.3 per indicator\n",
        "        \n",
        "        # Length bonus\n",
        "        if len(text) > 1000:\n",
        "            score += 0.2\n",
        "        elif len(text) > 500:\n",
        "            score += 0.1\n",
        "        \n",
        "        return builtins.min(score, 1.0)\n",
        "    \n",
        "    def _calculate_paragraph_confidence(self, content: str) -> float:\n",
        "        \"\"\"Calculate confidence score for a paragraph.\"\"\"\n",
        "        score = 0.0\n",
        "        \n",
        "        # Legal content indicators\n",
        "        legal_terms = ['court', 'chamber', 'prosecutor', 'defendant', 'evidence', 'witness', 'crime']\n",
        "        legal_count = sum(1 for term in legal_terms if term.lower() in content.lower())\n",
        "        score += builtins.min(legal_count * 0.1, 0.4)\n",
        "        \n",
        "        # Length penalty for very short content\n",
        "        if len(content) < 50:\n",
        "            score -= 0.2\n",
        "        \n",
        "        # Citation patterns\n",
        "        if re.search(r'ICC-\\d+/\\d+', content):\n",
        "            score += 0.2\n",
        "        \n",
        "        return builtins.max(0.0, builtins.min(score, 1.0))\n",
        "    \n",
        "    def _calculate_document_quality(self, pages: List[ParsedPage], paragraphs: List[LegalParagraph]) -> float:\n",
        "        \"\"\"Calculate overall document quality score.\"\"\"\n",
        "        if not pages:\n",
        "            return 0.0\n",
        "        \n",
        "        # Average page quality\n",
        "        avg_page_quality = sum(p.extraction_quality for p in pages) / len(pages)\n",
        "        \n",
        "        # Paragraph quality bonus\n",
        "        if paragraphs:\n",
        "            avg_para_confidence = sum(p.extraction_confidence for p in paragraphs) / len(paragraphs)\n",
        "            quality_score = (avg_page_quality * 0.7) + (avg_para_confidence * 0.3)\n",
        "        else:\n",
        "            quality_score = avg_page_quality\n",
        "        \n",
        "        return builtins.min(quality_score * 100, 100.0)\n",
        "\n",
        "print(\"‚úÖ ICC PDF Parser class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Export and Integration Functions\n",
        "\n",
        "Functions to export parsed data and integrate with the chunking pipeline:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_parsed_document(parsed_doc: ParsedDocument, output_format: str = \"json\") -> str:\n",
        "    \"\"\"Export parsed document in various formats.\"\"\"\n",
        "    \n",
        "    if output_format.lower() == \"json\":\n",
        "        # Convert to JSON-serializable format\n",
        "        export_data = {\n",
        "            \"metadata\": asdict(parsed_doc.metadata),\n",
        "            \"pages\": [asdict(page) for page in parsed_doc.pages],\n",
        "            \"paragraphs\": [asdict(para) for para in parsed_doc.paragraphs],\n",
        "            \"sections\": parsed_doc.sections,\n",
        "            \"extraction_errors\": parsed_doc.extraction_errors\n",
        "        }\n",
        "        \n",
        "        output_path = f\"/tmp/parsed_document_{int(time.time())}.json\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        return output_path\n",
        "    \n",
        "    elif output_format.lower() == \"parquet\":\n",
        "        # Create Spark DataFrame and save as Parquet\n",
        "        spark = SparkSession.getActiveSession()\n",
        "        if spark is None:\n",
        "            spark = SparkSession.builder.appName(\"PDF_Parsing\").getOrCreate()\n",
        "        \n",
        "        # Convert pages to DataFrame\n",
        "        pages_data = []\n",
        "        for page in parsed_doc.pages:\n",
        "            pages_data.append({\n",
        "                \"page_number\": page.page_number,\n",
        "                \"text\": page.text,\n",
        "                \"text_length\": page.text_length,\n",
        "                \"word_count\": page.word_count,\n",
        "                \"section_type\": page.section_type,\n",
        "                \"has_images\": page.has_images,\n",
        "                \"image_count\": page.image_count,\n",
        "                \"extraction_quality\": page.extraction_quality,\n",
        "                \"processing_time\": page.processing_time\n",
        "            })\n",
        "        \n",
        "        pages_df = spark.createDataFrame(pages_data)\n",
        "        \n",
        "        output_path = f\"/tmp/parsed_pages_{int(time.time())}.parquet\"\n",
        "        pages_df.write.mode(\"overwrite\").parquet(output_path)\n",
        "        \n",
        "        return output_path\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported output format: {output_format}\")\n",
        "\n",
        "def create_spark_dataframe(parsed_doc: ParsedDocument) -> 'pyspark.sql.DataFrame':\n",
        "    \"\"\"Create Spark DataFrame from parsed document for integration with chunking pipeline.\"\"\"\n",
        "    spark = SparkSession.getActiveSession()\n",
        "    if spark is None:\n",
        "        spark = SparkSession.builder.appName(\"PDF_Parsing\").getOrCreate()\n",
        "    \n",
        "    # Prepare data for chunking pipeline\n",
        "    chunking_data = []\n",
        "    \n",
        "    for page in parsed_doc.pages:\n",
        "        # Create base record for each page\n",
        "        base_record = {\n",
        "            \"page_number\": page.page_number,\n",
        "            \"text\": page.text,\n",
        "            \"section_type\": page.section_type,\n",
        "            \"text_length\": page.text_length,\n",
        "            \"word_count\": page.word_count,\n",
        "            \"extraction_quality\": page.extraction_quality,\n",
        "            \"case_name\": parsed_doc.metadata.case_name,\n",
        "            \"case_number\": parsed_doc.metadata.case_number,\n",
        "            \"chamber\": parsed_doc.metadata.chamber,\n",
        "            \"date\": parsed_doc.metadata.date,\n",
        "            \"extraction_method\": parsed_doc.metadata.extraction_method\n",
        "        }\n",
        "        \n",
        "        # Add paragraph-specific data if available\n",
        "        page_paragraphs = [p for p in parsed_doc.paragraphs if p.page_number == page.page_number]\n",
        "        \n",
        "        if page_paragraphs:\n",
        "            # Create record for each paragraph\n",
        "            for para in page_paragraphs:\n",
        "                record = base_record.copy()\n",
        "                record.update({\n",
        "                    \"paragraph_id\": para.paragraph_id,\n",
        "                    \"paragraph_number\": para.paragraph_number,\n",
        "                    \"paragraph_content\": para.content,\n",
        "                    \"paragraph_word_count\": para.word_count,\n",
        "                    \"paragraph_char_count\": para.char_count,\n",
        "                    \"is_numbered\": para.is_numbered,\n",
        "                    \"extraction_confidence\": para.extraction_confidence\n",
        "                })\n",
        "                chunking_data.append(record)\n",
        "        else:\n",
        "            # No paragraphs found, use page-level data\n",
        "            record = base_record.copy()\n",
        "            record.update({\n",
        "                \"paragraph_id\": f\"page_{page.page_number}\",\n",
        "                \"paragraph_number\": None,\n",
        "                \"paragraph_content\": page.text,\n",
        "                \"paragraph_word_count\": page.word_count,\n",
        "                \"paragraph_char_count\": page.text_length,\n",
        "                \"is_numbered\": False,\n",
        "                \"extraction_confidence\": page.extraction_quality\n",
        "            })\n",
        "            chunking_data.append(record)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = spark.createDataFrame(chunking_data)\n",
        "    return df\n",
        "\n",
        "def save_to_delta_table(parsed_doc: ParsedDocument, table_name: str = None) -> str:\n",
        "    \"\"\"Save parsed document to Delta table for downstream processing.\"\"\"\n",
        "    spark = SparkSession.getActiveSession()\n",
        "    if spark is None:\n",
        "        spark = SparkSession.builder.appName(\"PDF_Parsing\").getOrCreate()\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = create_spark_dataframe(parsed_doc)\n",
        "    \n",
        "    # Use configured table name or default\n",
        "    if table_name is None:\n",
        "        table_name = get_databricks_path(\"parsed_document_table\")\n",
        "    \n",
        "    # Save to Delta table\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
        "    \n",
        "    print(f\"‚úÖ Parsed document saved to Delta table: {table_name}\")\n",
        "    print(f\"üìä Records: {df.count()}\")\n",
        "    \n",
        "    return table_name\n",
        "\n",
        "print(\"‚úÖ Export and integration functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Examples and Testing\n",
        "\n",
        "Examples of how to use the PDF parsing system:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Basic PDF Parsing\n",
        "def parse_icc_judgment(pdf_path: str, save_to_delta: bool = True) -> ParsedDocument:\n",
        "    \"\"\"Parse ICC judgment PDF and optionally save to Delta table.\"\"\"\n",
        "    \n",
        "    # Initialize parser\n",
        "    parser = ICCPDFParser()\n",
        "    \n",
        "    # Parse the PDF\n",
        "    parsed_doc = parser.parse_pdf(pdf_path)\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\nüìä PARSING RESULTS:\")\n",
        "    print(f\"   Case: {parsed_doc.metadata.case_name}\")\n",
        "    print(f\"   Number: {parsed_doc.metadata.case_number}\")\n",
        "    print(f\"   Chamber: {parsed_doc.metadata.chamber}\")\n",
        "    print(f\"   Date: {parsed_doc.metadata.date}\")\n",
        "    print(f\"   Pages: {parsed_doc.metadata.total_pages}\")\n",
        "    print(f\"   Text length: {parsed_doc.metadata.total_text_length:,} chars\")\n",
        "    print(f\"   Word count: {parsed_doc.metadata.total_word_count:,} words\")\n",
        "    print(f\"   Quality score: {parsed_doc.metadata.quality_score:.1f}/100\")\n",
        "    print(f\"   Processing time: {parsed_doc.metadata.processing_time:.2f}s\")\n",
        "    \n",
        "    # Section breakdown\n",
        "    print(f\"\\nüìã SECTIONS FOUND:\")\n",
        "    for section, pages in parsed_doc.sections.items():\n",
        "        print(f\"   {section}: {len(pages)} pages\")\n",
        "    \n",
        "    # Paragraph statistics\n",
        "    if parsed_doc.paragraphs:\n",
        "        print(f\"\\nüìù PARAGRAPHS EXTRACTED:\")\n",
        "        print(f\"   Total paragraphs: {len(parsed_doc.paragraphs)}\")\n",
        "        numbered_paras = [p for p in parsed_doc.paragraphs if p.is_numbered]\n",
        "        print(f\"   Numbered paragraphs: {len(numbered_paras)}\")\n",
        "        avg_confidence = sum(p.extraction_confidence for p in parsed_doc.paragraphs) / len(parsed_doc.paragraphs)\n",
        "        print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
        "    \n",
        "    # Errors\n",
        "    if parsed_doc.extraction_errors:\n",
        "        print(f\"\\n‚ö†Ô∏è  EXTRACTION ERRORS: {len(parsed_doc.extraction_errors)}\")\n",
        "        for error in parsed_doc.extraction_errors[:5]:  # Show first 5 errors\n",
        "            print(f\"   {error}\")\n",
        "    \n",
        "    # Save to Delta table if requested\n",
        "    if save_to_delta:\n",
        "        table_name = save_to_delta_table(parsed_doc)\n",
        "        print(f\"\\nüíæ Saved to Delta table: {table_name}\")\n",
        "    \n",
        "    return parsed_doc\n",
        "\n",
        "print(\"‚úÖ Basic parsing function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Parse the ICC Judgment\n",
        "pdf_path = PDF_SOURCE_PATH\n",
        "\n",
        "print(f\"üîç Parsing ICC Judgment: {pdf_path}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Parse the document\n",
        "parsed_document = parse_icc_judgment(pdf_path, save_to_delta=True)\n",
        "\n",
        "print(\"\\n‚úÖ PDF parsing complete!\")\n",
        "print(\"üìã Ready for next step: Chunking pipeline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration with Chunking Pipeline\n",
        "\n",
        "Functions to integrate parsed data with the chunking system:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_for_chunking(parsed_doc: ParsedDocument) -> 'pyspark.sql.DataFrame':\n",
        "    \"\"\"Prepare parsed document data for the chunking pipeline.\"\"\"\n",
        "    \n",
        "    # Create Spark DataFrame\n",
        "    df = create_spark_dataframe(parsed_doc)\n",
        "    \n",
        "    # Add additional fields needed for chunking\n",
        "    df = df.withColumn(\"document_id\", lit(f\"icc_{parsed_doc.metadata.case_number.replace('/', '_')}\"))\n",
        "    df = df.withColumn(\"extraction_timestamp\", current_timestamp())\n",
        "    df = df.withColumn(\"pipeline_stage\", lit(\"parsed\"))\n",
        "    \n",
        "    # Add quality indicators\n",
        "    df = df.withColumn(\"high_quality\", \n",
        "                      when(col(\"extraction_quality\") > 0.7, True).otherwise(False))\n",
        "    df = df.withColumn(\"has_legal_content\", \n",
        "                      when(col(\"paragraph_content\").contains(\"court\") | \n",
        "                           col(\"paragraph_content\").contains(\"chamber\") |\n",
        "                           col(\"paragraph_content\").contains(\"prosecutor\"), True).otherwise(False))\n",
        "    \n",
        "    print(f\"‚úÖ Prepared {df.count()} records for chunking pipeline\")\n",
        "    print(f\"üìä High quality records: {df.filter(col('high_quality')).count()}\")\n",
        "    print(f\"‚öñÔ∏è  Legal content records: {df.filter(col('has_legal_content')).count()}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def export_for_chunking(parsed_doc: ParsedDocument, output_table: str = None) -> str:\n",
        "    \"\"\"Export parsed data in format ready for chunking pipeline.\"\"\"\n",
        "    \n",
        "    # Prepare data\n",
        "    df = prepare_for_chunking(parsed_doc)\n",
        "    \n",
        "    # Use configured table name or default\n",
        "    if output_table is None:\n",
        "        output_table = get_databricks_path(\"parsed_for_chunking\")\n",
        "    \n",
        "    # Save to Delta table\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_table)\n",
        "    \n",
        "    print(f\"‚úÖ Data exported for chunking: {output_table}\")\n",
        "    \n",
        "    return output_table\n",
        "\n",
        "# Example: Prepare data for chunking\n",
        "if 'parsed_document' in locals():\n",
        "    print(\"üîÑ Preparing data for chunking pipeline...\")\n",
        "    chunking_table = export_for_chunking(parsed_document)\n",
        "    print(f\"üìã Next step: Use table '{chunking_table}' in chunking notebook\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Run the parsing example above first to prepare data for chunking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "This notebook provides a complete, isolated PDF parsing system for ICC judgments:\n",
        "\n",
        "### ‚úÖ What's Included\n",
        "- **PyMuPDF-based parser** (28.8x faster than pdfplumber)\n",
        "- **ICC-specific patterns** for legal document structure\n",
        "- **Quality assessment** with confidence scoring\n",
        "- **Multiple export formats** (JSON, Parquet, Delta)\n",
        "- **Spark integration** for downstream processing\n",
        "- **Error handling** and comprehensive logging\n",
        "\n",
        "### üîÑ Workflow Integration\n",
        "1. **Parse PDF** ‚Üí This notebook (00_PDF_Parsing_Isolation.ipynb)\n",
        "2. **Build Chunks** ‚Üí 03_ICC_Judgment_Chunking.ipynb\n",
        "3. **Summarize Chunks** ‚Üí 04_ICC_Judgment_Chunking_summary.ipynb\n",
        "4. **Optimize Vector Search** ‚Üí 01_Optimized_Vector_Search.ipynb\n",
        "5. **Build and Deploy RAG** ‚Üí 02_Production_RAG_Deployment.ipynb\n",
        "\n",
        "### üìä Performance Metrics\n",
        "- **Speed**: ~5 seconds for 1,616 pages (vs 141s with pdfplumber)\n",
        "- **Text extraction**: 4,475,751 characters\n",
        "- **Quality score**: Automated assessment of extraction quality\n",
        "- **Error handling**: Comprehensive error tracking and reporting\n",
        "\n",
        "### üöÄ Ready for Production\n",
        "The parsed data is automatically saved to Delta tables and ready for the next stage of the RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
