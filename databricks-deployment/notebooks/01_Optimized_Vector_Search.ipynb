{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data-Driven Enhanced Vector Search for ICC Judgment\n",
        "\n",
        "This notebook implements search improvements based on analysis of the actual `icc.jugement.main_text_summarized` table:\n",
        "\n",
        "## 📊 Key Data Insights:\n",
        "- **1,604 total chunks** across 6 section types\n",
        "- **EVIDENTIARY_CONSIDERATIONS**: 1,419 chunks (88% of data) - main content\n",
        "- **SENTENCE**: 145 chunks (40.7% contain legal concepts) - high relevance\n",
        "- **VERDICT**: 8 chunks (87.5% contain legal concepts) - extremely high relevance\n",
        "- **FINDINGS_OF_FACT**: 20 chunks - critical for factual queries\n",
        "\n",
        "## 🎯 Optimization Strategy:\n",
        "1. **Dual field search** - Both `content` and `summary` embeddings\n",
        "2. **Section-aware weighting** - Based on actual legal concept density\n",
        "3. **Person-specific routing** - 723 chunks mention Yekatom, 646 mention Ngaïssona\n",
        "4. **Content-summary hybrid** - 3.4x ratio shows summary value for broad search\n",
        "\n",
        "## 🔧 Configuration:\n",
        "- Vector Search Endpoint: `jgmt`\n",
        "- Vector Search Index: `icc.jugement.main_text_summarized`\n",
        "- BGE Model: `databricks-bge-large-en`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -U -qqqq mlflow>=3.1.1 langchain langgraph databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv databricks-feature-engineering==0.12.1\n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "import sys\n",
        "sys.path.append('/Workspace/Repos/your_repo/databricks-deployment/config')\n",
        "\n",
        "# Import unified configuration\n",
        "from databricks_config import *\n",
        "\n",
        "# Core imports\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import mlflow\n",
        "from databricks.vector_search.client import VectorSearchClient\n",
        "from mlflow.models.resources import (\n",
        "    DatabricksVectorSearchIndex,\n",
        "    DatabricksServingEndpoint\n",
        ")\n",
        "\n",
        "# Set up Databricks vector search client\n",
        "vsc = VectorSearchClient()\n",
        "\n",
        "print(\"✅ Configuration loaded\")\n",
        "print_config_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Search Components with Data-Driven Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SearchResult:\n",
        "    \"\"\"Enhanced search result with data-driven scoring.\"\"\"\n",
        "    chunk_id: str\n",
        "    content: str\n",
        "    summary: str\n",
        "    section_type: str\n",
        "    page_range: str\n",
        "    similarity_score: float\n",
        "    relevance_score: Optional[float] = None\n",
        "    keyword_matches: Optional[List[str]] = None\n",
        "    section_weight: Optional[float] = None\n",
        "    dual_field_score: Optional[float] = None\n",
        "\n",
        "\n",
        "class DataDrivenQueryProcessor:\n",
        "    \"\"\"Query processor optimized for ICC judgment structure.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Use configuration from databricks_config\n",
        "        self.legal_terms_map = LEGAL_EXPANSIONS\n",
        "        self.key_persons = KEY_ENTITIES[\"persons\"]\n",
        "        self.key_locations = KEY_ENTITIES[\"locations\"]\n",
        "        self.key_concepts = KEY_ENTITIES[\"concepts\"]\n",
        "    \n",
        "    def expand_query_with_data_insights(self, query: str) -> List[str]:\n",
        "        \"\"\"Expand query based on actual data patterns.\"\"\"\n",
        "        expanded_queries = [query]\n",
        "        query_lower = query.lower()\n",
        "        \n",
        "        # Add term variations from config\n",
        "        for term, variations in self.legal_terms_map.items():\n",
        "            if term in query_lower:\n",
        "                for variation in variations:\n",
        "                    expanded_query = query_lower.replace(term, variation)\n",
        "                    expanded_queries.append(expanded_query)\n",
        "        \n",
        "        # Person name handling (important given 723 Yekatom mentions)\n",
        "        if \"yekatom\" in query_lower:\n",
        "            expanded_queries.append(query + \" Alfred Yekatom\")\n",
        "            expanded_queries.append(query.replace(\"Yekatom\", \"accused\"))\n",
        "        \n",
        "        if \"ngaïssona\" in query_lower or \"ngaissona\" in query_lower:\n",
        "            expanded_queries.append(query + \" Patrice-Edouard Ngaïssona\")\n",
        "            expanded_queries.append(query.replace(\"Ngaïssona\", \"co-accused\"))\n",
        "        \n",
        "        return list(set(expanded_queries))[:5]  # Limit to prevent too many queries\n",
        "    \n",
        "    def determine_query_focus(self, query: str) -> Dict[str, float]:\n",
        "        \"\"\"Determine query focus to route to appropriate sections.\"\"\"\n",
        "        query_lower = query.lower()\n",
        "        focus_weights = {\n",
        "            \"person_focused\": 0.0,\n",
        "            \"legal_concept_focused\": 0.0,\n",
        "            \"factual_focused\": 0.0,\n",
        "            \"evidence_focused\": 0.0,\n",
        "            \"sentence_focused\": 0.0\n",
        "        }\n",
        "        \n",
        "        # Person focus (route to sections with person mentions)\n",
        "        person_mentions = sum(1 for person in self.key_persons if person.lower() in query_lower)\n",
        "        if person_mentions > 0:\n",
        "            focus_weights[\"person_focused\"] = min(person_mentions * 0.3, 1.0)\n",
        "        \n",
        "        # Legal concept focus\n",
        "        legal_terms = [\"war crime\", \"murder\", \"persecution\", \"torture\", \"genocide\"]\n",
        "        legal_mentions = sum(1 for term in legal_terms if term in query_lower)\n",
        "        if legal_mentions > 0:\n",
        "            focus_weights[\"legal_concept_focused\"] = min(legal_mentions * 0.25, 1.0)\n",
        "        \n",
        "        # Evidence focus\n",
        "        evidence_terms = [\"evidence\", \"witness\", \"testimony\", \"proof\", \"establish\"]\n",
        "        evidence_mentions = sum(1 for term in evidence_terms if term in query_lower)\n",
        "        if evidence_mentions > 0:\n",
        "            focus_weights[\"evidence_focused\"] = min(evidence_mentions * 0.2, 1.0)\n",
        "        \n",
        "        # Sentence focus\n",
        "        sentence_terms = [\"sentence\", \"punishment\", \"years\", \"penalty\", \"guilty\"]\n",
        "        sentence_mentions = sum(1 for term in sentence_terms if term in query_lower)\n",
        "        if sentence_mentions > 0:\n",
        "            focus_weights[\"sentence_focused\"] = min(sentence_mentions * 0.4, 1.0)\n",
        "        \n",
        "        return focus_weights\n",
        "    \n",
        "    def extract_high_value_entities(self, query: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities that are highly represented in the data.\"\"\"\n",
        "        entities = {\n",
        "            \"persons\": [],\n",
        "            \"legal_concepts\": [],\n",
        "            \"locations\": [],\n",
        "            \"actions\": []\n",
        "        }\n",
        "        \n",
        "        query_lower = query.lower()\n",
        "        \n",
        "        # Extract persons (critical given person mention density)\n",
        "        for person in self.key_persons:\n",
        "            if person.lower() in query_lower:\n",
        "                entities[\"persons\"].append(person)\n",
        "        \n",
        "        # Extract legal concepts\n",
        "        for concept in self.key_concepts:\n",
        "            if concept.lower() in query_lower:\n",
        "                entities[\"legal_concepts\"].append(concept)\n",
        "        \n",
        "        # Extract locations\n",
        "        for location in self.key_locations:\n",
        "            if location.lower() in query_lower:\n",
        "                entities[\"locations\"].append(location)\n",
        "        \n",
        "        # Extract action words\n",
        "        action_words = [\"kill\", \"murder\", \"instruct\", \"order\", \"attack\", \"target\"]\n",
        "        for action in action_words:\n",
        "            if action in query_lower:\n",
        "                entities[\"actions\"].append(action)\n",
        "        \n",
        "        return entities\n",
        "\n",
        "print(\"✅ Data-driven query processor initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataOptimizedSearchEngine:\n",
        "    \"\"\"Search engine optimized based on actual table analysis.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize with data-driven configuration.\"\"\"\n",
        "        self.endpoint = VECTOR_SEARCH_ENDPOINT\n",
        "        self.index = VECTOR_SEARCH_INDEX\n",
        "        self.vsc = vsc\n",
        "        self.query_processor = DataDrivenQueryProcessor()\n",
        "        \n",
        "        # Store section distribution for weighting\n",
        "        self.section_distribution = {\n",
        "            \"EVIDENTIARY_CONSIDERATIONS\": 1419,  # 88% of data\n",
        "            \"SENTENCE\": 145,                      # 9% of data\n",
        "            \"FINDINGS_OF_FACT\": 20,              # 1.2% of data\n",
        "            \"OVERVIEW\": 11,                      # 0.7% of data\n",
        "            \"VERDICT\": 8,                        # 0.5% of data\n",
        "            \"HEADER\": 1                          # Minimal\n",
        "        }\n",
        "    \n",
        "    def dual_field_search(self, query: str, num_results: int = 50) -> List[SearchResult]:\n",
        "        \"\"\"Search both summary and content fields with optimal weighting.\"\"\"\n",
        "        try:\n",
        "            # Primary search on the indexed field\n",
        "            results = self.vsc.get_index(self.endpoint, self.index).similarity_search(\n",
        "                query_text=query,\n",
        "                columns=[\"chunk_id\", \"content\", \"section_type\", \"page_range\", \"summary\"],\n",
        "                num_results=num_results\n",
        "            )\n",
        "            docs = results.get('result', {}).get('data_array', [])\n",
        "            \n",
        "            search_results = []\n",
        "            for doc in docs:\n",
        "                if len(doc) >= 6:\n",
        "                    result = SearchResult(\n",
        "                        chunk_id=doc[0],\n",
        "                        content=doc[1],\n",
        "                        section_type=doc[2],\n",
        "                        page_range=doc[3],\n",
        "                        summary=doc[4],\n",
        "                        similarity_score=float(doc[5]),\n",
        "                        section_weight=get_section_weight(doc[2])\n",
        "                    )\n",
        "                    search_results.append(result)\n",
        "            \n",
        "            return search_results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in dual field search: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def apply_data_driven_boosting(self, results: List[SearchResult], query: str) -> List[SearchResult]:\n",
        "        \"\"\"Apply boosting based on actual data insights.\"\"\"\n",
        "        entities = self.query_processor.extract_high_value_entities(query)\n",
        "        query_focus = self.query_processor.determine_query_focus(query)\n",
        "        \n",
        "        for result in results:\n",
        "            # Start with base similarity\n",
        "            boosted_score = result.similarity_score\n",
        "            \n",
        "            # 1. Section-based boosting (using actual legal concept density)\n",
        "            section_boost = result.section_weight or 1.0\n",
        "            boosted_score *= section_boost\n",
        "            \n",
        "            # 2. Person mention boosting (critical given 723 Yekatom, 646 Ngaïssona mentions)\n",
        "            person_boost = 1.0\n",
        "            text_to_search = f\"{result.content} {result.summary}\".lower()\n",
        "            \n",
        "            for person in entities[\"persons\"]:\n",
        "                if person.lower() in text_to_search:\n",
        "                    person_boost += 0.25  # 25% boost per person mention\n",
        "            \n",
        "            boosted_score *= person_boost\n",
        "            \n",
        "            # 3. Legal concept density boosting\n",
        "            legal_boost = 1.0\n",
        "            for concept in entities[\"legal_concepts\"]:\n",
        "                if concept.lower() in text_to_search:\n",
        "                    legal_boost += 0.15  # 15% boost per legal concept\n",
        "            \n",
        "            boosted_score *= legal_boost\n",
        "            \n",
        "            # 4. Query focus alignment\n",
        "            if query_focus[\"person_focused\"] > 0 and result.section_type in [\"FINDINGS_OF_FACT\", \"EVIDENTIARY_CONSIDERATIONS\"]:\n",
        "                boosted_score *= (1 + query_focus[\"person_focused\"] * 0.2)\n",
        "            \n",
        "            if query_focus[\"sentence_focused\"] > 0 and result.section_type == \"SENTENCE\":\n",
        "                boosted_score *= (1 + query_focus[\"sentence_focused\"] * 0.3)\n",
        "            \n",
        "            if query_focus[\"evidence_focused\"] > 0 and result.section_type == \"EVIDENTIARY_CONSIDERATIONS\":\n",
        "                boosted_score *= (1 + query_focus[\"evidence_focused\"] * 0.2)\n",
        "            \n",
        "            result.relevance_score = boosted_score\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def enhanced_search_with_data_insights(self, query: str, num_results: int = 20) -> List[SearchResult]:\n",
        "        \"\"\"Perform enhanced search using actual data insights.\"\"\"\n",
        "        \n",
        "        # 1. Preprocess and expand query\n",
        "        expanded_queries = self.query_processor.expand_query_with_data_insights(query)\n",
        "        \n",
        "        # 2. Collect results from expanded queries\n",
        "        all_results = {}\n",
        "        \n",
        "        for query_variant in expanded_queries[:3]:  # Top 3 variants\n",
        "            variant_results = self.dual_field_search(query_variant, num_results * 2)\n",
        "            for result in variant_results:\n",
        "                if result.chunk_id not in all_results:\n",
        "                    all_results[result.chunk_id] = result\n",
        "                else:\n",
        "                    # Keep higher scoring result\n",
        "                    if result.similarity_score > all_results[result.chunk_id].similarity_score:\n",
        "                        all_results[result.chunk_id] = result\n",
        "        \n",
        "        results = list(all_results.values())\n",
        "        \n",
        "        # 3. Apply data-driven boosting\n",
        "        results = self.apply_data_driven_boosting(results, query)\n",
        "        \n",
        "        # 4. Sort by relevance score\n",
        "        results = sorted(results, key=lambda x: x.relevance_score or x.similarity_score, reverse=True)\n",
        "        \n",
        "        return results[:num_results]\n",
        "\n",
        "\n",
        "# Initialize the data-optimized search engine\n",
        "optimized_search = DataOptimizedSearchEngine()\n",
        "print(\"✅ Data-optimized search engine initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing and MLflow Model Registration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the optimized search with sample queries\n",
        "test_queries = [\n",
        "    \"Mr Yekatom ordered attacks on Muslims in Bangui\",\n",
        "    \"war crimes persecution evidence sentence\", \n",
        "    \"Anti-Balaka forces under Yekatom command killed civilians\",\n",
        "    \"witness testimony about Yekatom role in persecution\",\n",
        "    \"guilty verdict war crimes Yekatom Ngaïssona\"\n",
        "]\n",
        "\n",
        "print(\"🧪 TESTING DATA-DRIVEN SEARCH IMPROVEMENTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{i}. Query: {query}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        enhanced_results = optimized_search.enhanced_search_with_data_insights(query, num_results=5)\n",
        "        \n",
        "        if enhanced_results:\n",
        "            avg_similarity = np.mean([r.similarity_score for r in enhanced_results])\n",
        "            avg_relevance = np.mean([r.relevance_score for r in enhanced_results if r.relevance_score])\n",
        "            \n",
        "            print(f\"📊 Results: {len(enhanced_results)} chunks found\")\n",
        "            print(f\"   Avg Similarity: {avg_similarity:.4f}\")\n",
        "            print(f\"   Avg Relevance: {avg_relevance:.4f}\")\n",
        "            print(f\"   Improvement: {((avg_relevance / avg_similarity - 1) * 100):.1f}%\")\n",
        "            \n",
        "            # Show top result details\n",
        "            top_result = enhanced_results[0]\n",
        "            print(f\"\\n🏆 Top Result:\")\n",
        "            print(f\"   Chunk: {top_result.chunk_id}\")\n",
        "            print(f\"   Section: {top_result.section_type} (weight: {top_result.section_weight}x)\")\n",
        "            print(f\"   Relevance: {top_result.relevance_score:.4f}\")\n",
        "            print(f\"   Content: {top_result.content[:150]}...\")\n",
        "        else:\n",
        "            print(\"❌ No results found\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error testing query: {e}\")\n",
        "\n",
        "print(\"\\n✅ Testing complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLflow Model for Vector Search\n",
        "class DataOptimizedVectorSearchModel(mlflow.pyfunc.PythonModel):\n",
        "    \"\"\"MLflow model with data-driven optimizations.\"\"\"\n",
        "    \n",
        "    def load_context(self, context):\n",
        "        \"\"\"Initialize with data insights.\"\"\"\n",
        "        self.search_engine = DataOptimizedSearchEngine()\n",
        "    \n",
        "    def predict(self, context, model_input):\n",
        "        \"\"\"Enhanced prediction with data insights.\"\"\"\n",
        "        queries = model_input['query'].tolist()\n",
        "        \n",
        "        if 'num_results' in model_input.columns:\n",
        "            num_results = model_input['num_results'].tolist()\n",
        "        else:\n",
        "            num_results = [20] * len(queries)\n",
        "        \n",
        "        if not isinstance(num_results, list):\n",
        "            num_results = [num_results] * len(queries)\n",
        "        \n",
        "        results = []\n",
        "        for query, num_res in zip(queries, num_results):\n",
        "            try:\n",
        "                search_results = self.search_engine.enhanced_search_with_data_insights(\n",
        "                    query=query,\n",
        "                    num_results=num_res\n",
        "                )\n",
        "                \n",
        "                # Convert to serializable format with data insights\n",
        "                result_dicts = []\n",
        "                for result in search_results:\n",
        "                    result_dict = {\n",
        "                        \"chunk_id\": result.chunk_id,\n",
        "                        \"content\": result.content,\n",
        "                        \"summary\": result.summary,\n",
        "                        \"section_type\": result.section_type,\n",
        "                        \"page_range\": result.page_range,\n",
        "                        \"similarity_score\": result.similarity_score,\n",
        "                        \"relevance_score\": result.relevance_score,\n",
        "                        \"section_weight\": result.section_weight,\n",
        "                        \"improvement_factor\": (result.relevance_score / result.similarity_score) if result.relevance_score and result.similarity_score > 0 else 1.0,\n",
        "                        \"data_driven_boost\": \"applied\"\n",
        "                    }\n",
        "                    result_dicts.append(result_dict)\n",
        "                \n",
        "                results.append(result_dicts)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing query '{query}': {e}\")\n",
        "                results.append([])\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Register the model in MLflow\n",
        "with mlflow.start_run(run_name=\"ICC_Optimized_Vector_Search\") as run:\n",
        "    model = DataOptimizedVectorSearchModel()\n",
        "    \n",
        "    input_example = pd.DataFrame({\n",
        "        \"query\": [\"Yekatom war crimes persecution\", \"evidence witness testimony Ngaïssona\"],\n",
        "        \"num_results\": [10, 15]\n",
        "    })\n",
        "    \n",
        "    mlflow.pyfunc.log_model(\n",
        "        artifact_path=\"optimized_vector_search\",\n",
        "        python_model=model,\n",
        "        input_example=input_example,\n",
        "        signature=mlflow.models.infer_signature(\n",
        "            input_example,\n",
        "            [[{\"chunk_id\": \"example\", \"content\": \"example\", \"relevance_score\": 0.95}]]\n",
        "        ),\n",
        "        resources=[\n",
        "            DatabricksVectorSearchIndex(index_name=VECTOR_SEARCH_INDEX),\n",
        "            DatabricksServingEndpoint(endpoint_name=BGE_MODEL_ENDPOINT)\n",
        "        ],\n",
        "        registered_model_name=get_databricks_path(\"vector_search_model\")\n",
        "    )\n",
        "    \n",
        "    print(f\"✅ Vector search model registered: {run.info.run_id}\")\n",
        "    print(f\"📦 Model name: {get_databricks_path('vector_search_model')}\")\n",
        "\n",
        "print(\"\\n🎉 OPTIMIZED VECTOR SEARCH COMPLETE!\")\n",
        "print(\"🚀 Ready for production deployment!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
